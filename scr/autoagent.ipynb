{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd4da5e",
   "metadata": {},
   "source": [
    "# Muestra básica de AutoGen\n",
    "\n",
    "En este ejemplo de código, utilizará el [AutoGen](https://aka.ms/ai-agents/autogen) Marco de IA para crear un agente básico.\n",
    "\n",
    "El objetivo de este ejemplo es mostrarle los pasos que usaremos más adelante en los ejemplos de código adicionales al implementar los diferentes patrones de agencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cb064",
   "metadata": {},
   "source": [
    "## Import the Needed Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e393f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.azure import AzureAIChatCompletionClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.ui import Console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403174f8",
   "metadata": {},
   "source": [
    "## Create the Client \n",
    "\n",
    "En este ejemplo, utilizaremos[GitHub Models](https://aka.ms/ai-agents-beginners/github-models) para acceder al LLM. \n",
    "\n",
    "El `model` esta definidocomo  `gpt-4o-mini`. TPrueba a cambiarlo por otro disponible en la tienda de modelos de GitHub para ver los diferentes resultados.\n",
    "\n",
    "Como prueba rápida, simplemente ejecutaremos un mensaje simple. - `¿?`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b0fbb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientAuthenticationError",
     "evalue": "(unauthorized) Bad credentials\nCode: unauthorized\nMessage: Bad credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientAuthenticationError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m load_dotenv()\n\u001b[32m      2\u001b[39m client = AzureAIChatCompletionClient(\n\u001b[32m      3\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     endpoint=\u001b[33m\"\u001b[39m\u001b[33mhttps://models.inference.ai.azure.com\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     },\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m client.create([UserMessage(content=\u001b[33m\"\u001b[39m\u001b[33m¿Cuál es la capital de Francia?\u001b[39m\u001b[33m\"\u001b[39m, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\autogen_ext\\models\\azure\\_azure_ai_client.py:333\u001b[39m, in \u001b[36mAzureAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m     cancellation_token.link_future(task)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m result: ChatCompletions = \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[32m    335\u001b[39m usage = RequestUsage(\n\u001b[32m    336\u001b[39m     prompt_tokens=result.usage.prompt_tokens \u001b[38;5;28;01mif\u001b[39;00m result.usage \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    337\u001b[39m     completion_tokens=result.usage.completion_tokens \u001b[38;5;28;01mif\u001b[39;00m result.usage \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    338\u001b[39m )\n\u001b[32m    340\u001b[39m choice = result.choices[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\azure\\ai\\inference\\aio\\_patch.py:675\u001b[39m, in \u001b[36mChatCompletionsClient.complete\u001b[39m\u001b[34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[39m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[32m    674\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m response.read()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response)\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[39m, in \u001b[36mmap_error\u001b[39m\u001b[34m(status_code, response, error_map)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    162\u001b[39m error = error_type(response=response)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mClientAuthenticationError\u001b[39m: (unauthorized) Bad credentials\nCode: unauthorized\nMessage: Bad credentials"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "client = AzureAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    endpoint=\"https://models.inference.ai.azure.com\",\n",
    "    # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n",
    "    # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n",
    "    credential=AzureKeyCredential(os.getenv(\"GITHUB_TOKEN\")),\n",
    "    model_info={\n",
    "        \"json_output\": True,\n",
    "        \"function_calling\": True,\n",
    "        \"vision\": True,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "\n",
    "result = await client.create([UserMessage(content=\"¿Cuál es la capital de Francia?\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9206de",
   "metadata": {},
   "source": [
    "## Definición del agente\n",
    "\n",
    "Ahora que hemos configurado el `client` y confirmado que funciona, vamos a crear un `AssistantAgent`. EA cada agente se le puede asignar: \n",
    "**name** - Un nombre abreviado que será útil para referenciarlo en flujos de múltiples agentes.\n",
    "**model_client** - El cliente que creó en el paso anterior.\n",
    "**tools** - Herramientas disponibles que el Agente puede utilizar para completar una tarea.\n",
    "**system_message** - El metaprompt que define la tarea, el comportamiento y el tono del LLM.\n",
    "\n",
    "Puedes cambiar el mensaje del sistema para ver cómo responde el LLM. Abordaremos este tema. `tools` en la lección #4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e10fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=client,\n",
    "    tools=[],\n",
    "    system_message=\"Eres VitalMind, un agente experto en salud. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379a0b0",
   "metadata": {},
   "source": [
    "## Ejecutar el agente\n",
    "\n",
    "La siguiente función ejecutará el agente. Usamos el `on_message` método para actualizar el estado del Agente con el nuevo mensaje. \n",
    "\n",
    "En este caso, actualizamos el estado con un nuevo mensaje del usuario que es `\"Planifícame una rutina de alimentacion saludable\"`.\n",
    "\n",
    "Puede cambiar el contenido del mensaje para ver cómo el LLM responde de manera diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517bb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "async def assistant_run():\n",
    "    # Define the query\n",
    "    user_query = \"Planifícame una rutina de alimentacion saludable\"\n",
    "\n",
    "    # Start building HTML output\n",
    "    html_output = \"<div style='margin-bottom:10px'>\"\n",
    "    html_output += \"<div style='font-weight:bold'>User:</div>\"\n",
    "    html_output += f\"<div style='margin-left:20px'>{user_query}</div>\"\n",
    "    html_output += \"</div>\"\n",
    "\n",
    "    # Execute the agent response\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=user_query, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "\n",
    "    # Add agent response to HTML\n",
    "    html_output += \"<div style='margin-bottom:20px'>\"\n",
    "    html_output += \"<div style='font-weight:bold'>Assistant:</div>\"\n",
    "    html_output += f\"<div style='margin-left:20px; white-space:pre-wrap'>{response.chat_message.content}</div>\"\n",
    "    html_output += \"</div>\"\n",
    "\n",
    "    # Display formatted HTML\n",
    "    display(HTML(html_output))\n",
    "\n",
    "# Run the function\n",
    "await assistant_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
